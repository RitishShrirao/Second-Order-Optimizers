{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),]\n",
    ")\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# data_path = '../../../../autoencoder/'\n",
    "data_path = './'\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=data_path, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=16)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root=data_path, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=16)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28) (60000,)\n",
      "(10000, 1, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Convert pytorch dataset to numpy array\n",
    "def get_data(dataset):\n",
    "    data = []\n",
    "    train_labels = []\n",
    "    for i, (inputs, labels) in enumerate(dataset):\n",
    "        data.append(inputs)\n",
    "        train_labels.append(labels)\n",
    "    return torch.cat(data).numpy(), torch.cat(train_labels).numpy()\n",
    "\n",
    "train_data, train_labels = get_data(trainloader)\n",
    "test_data, test_labels = get_data(testloader)\n",
    "\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot train_data sample\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(train_labels[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot test_data sample\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(test_data[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(test_labels[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an autograd engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros_like(data)\n",
    "        self.grad2 = np.zeros_like(data)\n",
    "        self._backward = lambda: None\n",
    "        self._backward_second_order = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self.requires_grad = True\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            out = Tensor(self.data + other.data, (self, other), '+', label=f'{self.label}+{other.label}')\n",
    "        else:\n",
    "            out = Tensor(self.data + other, (self,), '+')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                    self.grad2 = out.grad2 + self.grad2\n",
    "                    if isinstance(other, Tensor) and other.requires_grad:\n",
    "                         other.grad2 = out.grad2 + other.grad2\n",
    "                else:\n",
    "                    self.grad = out.grad + self.grad\n",
    "                    if isinstance(other, Tensor) and other.requires_grad:\n",
    "                        other.grad = out.grad + other.grad\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            out = Tensor(self.data - other.data, (self, other), '-', label=f'{self.label}-{other.label}')\n",
    "        else:\n",
    "            out = Tensor(self.data - other, (self,), '-')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                    self.grad2 = out.grad2 + self.grad2\n",
    "                    if isinstance(other, Tensor) and other.requires_grad:\n",
    "                         other.grad2 = -out.grad2 + other.grad2\n",
    "                else:\n",
    "                    self.grad = out.grad + self.grad\n",
    "                    if isinstance(other, Tensor) and other.requires_grad:\n",
    "                        other.grad = -out.grad + other.grad\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            out = Tensor(self.data * other.data, (self, other), '*', label=f'{self.label}*{other.label}')\n",
    "        else:\n",
    "            out = Tensor(self.data * other, (self,), '*')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                    if isinstance(other, Tensor):\n",
    "                         self.grad2 = out.grad2 * other.data + self.grad2\n",
    "                         if other.requires_grad:\n",
    "                            other.grad2 = out.grad2 * self.data + other.grad2\n",
    "                    else:\n",
    "                         self.grad2 = out.grad2 * other + self.grad2\n",
    "                else:    \n",
    "                    if isinstance(other, Tensor):\n",
    "                        self.grad = out.grad * other.data + self.grad\n",
    "                        if other.requires_grad:\n",
    "                            other.grad = out.grad * self.data + other.grad\n",
    "                    else:\n",
    "                        self.grad = out.grad * other + self.grad\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        out = Tensor(data, (self,), 'getitem', label=f'{self.label}[{index}]')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                     self.grad2[index] = out.grad2 + self.grad2[index]\n",
    "                else:\n",
    "                    self.grad[index] = out.grad + self.grad[index]\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return Tensor(-self.data, (self,), 'neg', label=f'-{self.label}')\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return -self + other\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __pow__(self, exponent: float):\n",
    "        data = self.data**exponent\n",
    "        out = Tensor(data, (self,), f'**{exponent}', label=f'{self.label}**{exponent}')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                    self.grad2 = out.grad2 * exponent * (exponent-1) * self.data ** (exponent - 2) + self.grad2\n",
    "                else:\n",
    "                    self.grad = out.grad * exponent * self.data ** (exponent - 1) + self.grad\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    def dot(self, other):\n",
    "        out_data = np.matmul(self.data, other.data)\n",
    "        out = Tensor(out_data, (self, other), 'matmul', label=f'{self.label} matmul {other.label}')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                    if isinstance(out.grad2, Tensor):\n",
    "                        self.grad2 = out.grad2.dot(other.swapaxes(-1,-2)) + self.grad2\n",
    "                        if other.requires_grad:\n",
    "                            other.grad2 = self.swapaxes(-2,-1).dot(out.grad2) + other.grad2\n",
    "                    else:\n",
    "                        self.grad2 += np.matmul(out.grad2, other.data.swapaxes(-1, -2))\n",
    "                        if other.requires_grad:\n",
    "                            other.grad2 += np.matmul(self.data.swapaxes(-2, -1), out.grad2)\n",
    "                else:\n",
    "                    if isinstance(out.grad, Tensor):\n",
    "                        self.grad = out.grad.dot(other.swapaxes(-1,-2)) + self.grad\n",
    "                        if other.requires_grad:\n",
    "                            other.grad = self.swapaxes(-2,-1).dot(out.grad) + other.grad\n",
    "                    else:\n",
    "                        self.grad += np.matmul(out.grad, other.data.swapaxes(-1, -2))\n",
    "                        if other.requires_grad:\n",
    "                            other.grad += np.matmul(self.data.swapaxes(-2, -1), out.grad)\n",
    "                \n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "\n",
    "    def T(self, axes=None):\n",
    "        data = np.transpose(self.data, axes)\n",
    "        out = Tensor(data, (self,), 'transpose', label=f'{self.label}.T')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if axes is None:\n",
    "                    axes_inv = None\n",
    "                else:\n",
    "                    axes_inv = np.argsort(axes)\n",
    "                \n",
    "                if secondOrder:\n",
    "                    if isinstance(out.grad2, Tensor):\n",
    "                         self.grad2 = out.grad2.T(axes_inv) + self.grad2\n",
    "                    else:\n",
    "                         self.grad2 += np.transpose(out.grad2, axes_inv)\n",
    "                else:\n",
    "                    if isinstance(out.grad, Tensor):\n",
    "                        self.grad = out.grad.T(axes_inv) + self.grad\n",
    "                    else:\n",
    "                        self.grad += np.transpose(out.grad, axes_inv)\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "    \n",
    "    def swapaxes(self, axis1, axis2):\n",
    "        data = np.swapaxes(self.data, axis1, axis2)\n",
    "        out = Tensor(data, (self,), 'swapaxes', label=f'{self.label}.swapaxes({axis1}, {axis2})')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                    self.grad2 = out.grad2.swapaxes(axis1, axis2) + self.grad2\n",
    "                else:\n",
    "                    self.grad = out.grad.swapaxes(axis1, axis2) + self.grad\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "\n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        data = self.data.sum(axis=axis, keepdims=keepdims)\n",
    "        out = Tensor(data, (self,), 'sum', label=f'{self.label} sum')\n",
    "\n",
    "        def _backward(axis=axis, secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                grad_shape = np.ones_like(self.data.shape)\n",
    "                if axis is not None:\n",
    "                    if isinstance(axis, int):\n",
    "                        axis = (axis,)\n",
    "                    for ax in axis:\n",
    "                        grad_shape[ax] = self.data.shape[ax]\n",
    "                \n",
    "                if secondOrder:\n",
    "                    self.grad2 = out.grad2 * np.ones_like(self.data) + self.grad2\n",
    "                else:\n",
    "                    self.grad = out.grad * np.ones_like(self.data) + self.grad\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "    \n",
    "    def flatten(self):\n",
    "        data = self.data.flatten()\n",
    "        out = Tensor(data, (self,), 'flatten', label=f'{self.label} flatten')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                    self.grad2 = out.grad2.reshape(self.data.shape) + self.grad2\n",
    "                else:\n",
    "                    self.grad = out.grad.reshape(self.data.shape) + self.grad\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        data = np.maximum(0, self.data)\n",
    "        out = Tensor(data, (self,), 'relu', label=f'{self.label} relu')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                    self.grad2 = self.grad2\n",
    "                else:\n",
    "                    self.grad = out.grad * (self.data > 0) + self.grad\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "    \n",
    "    def leaky_relu(self):\n",
    "        data = np.maximum(0.01 * self.data, self.data)\n",
    "        out = Tensor(data, (self,), 'leaky relu', label=f'{self.label} leaky relu')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                    self.grad2 = self.grad2\n",
    "                else:\n",
    "                    self.grad = out.grad * (self.data > 0) +  out.grad * (0.01 * (self.data < 0))\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        data = np.tanh(self.data)\n",
    "        out = Tensor(data, (self,), 'tanh', label=f'{self.label} tanh')\n",
    "\n",
    "        def _backward(secondOrder=False):\n",
    "            if self.requires_grad:\n",
    "                if secondOrder:\n",
    "                    self.grad2 = -2 * out.grad2 * data * (1-data**2) + self.grad2\n",
    "                else:\n",
    "                    self.grad = out.grad * (1 - data**2) + self.grad\n",
    "\n",
    "        out._backward = partial(_backward, secondOrder=False)\n",
    "        out._backward_second_order = partial(_backward, secondOrder=True)\n",
    "        return out\n",
    "\n",
    "    def backward(self, secondOrder=False, hessian=False):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        if not hessian:\n",
    "            self.grad = (1.0 * np.ones_like(self.data))\n",
    "            if secondOrder:\n",
    "                self.grad = Tensor(self.grad, label='grad')\n",
    "                self.grad2 = 1.0 * np.ones_like(self.data)\n",
    "                \n",
    "            for node in reversed(topo):\n",
    "                node._backward()\n",
    "\n",
    "        else:\n",
    "            self.grad2 = (1.0 * np.ones_like(self.data))\n",
    "            for node in reversed(topo):\n",
    "                node._backward_second_order()\n",
    "\n",
    "\n",
    "    def get_hessian(self):\n",
    "        \"\"\"Get the Hessian of this tensor w.r.t. another tensor out.\"\"\"\n",
    "        \n",
    "        n = self.data.shape[0]\n",
    "        hessian = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            # Do backward pass wrt i-th element of the gradient\n",
    "            self.grad[i].backward(hessian=True)\n",
    "\n",
    "            hessian[:,i] = self.grad2.squeeze()\n",
    "\n",
    "            self.grad[i].zero_grad2()\n",
    "\n",
    "        return hessian\n",
    "\n",
    "    def zero_grad2(self):\n",
    "        \"\"\"Resets gradients of this tensor and all its descendants to zero.\"\"\"\n",
    "        self.grad2 = np.zeros_like(self.data)\n",
    "\n",
    "        for child in self._prev:\n",
    "            if isinstance(child, Tensor) and child.requires_grad:\n",
    "                child.zero_grad2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient\n",
      "[[ 2.25260869]\n",
      " [-2.4673024 ]\n",
      " [-2.24779357]\n",
      " [ 0.56590759]\n",
      " [-8.06834315]]\n",
      "[[ 2.25260869]\n",
      " [-2.4673024 ]\n",
      " [-2.24779357]\n",
      " [ 0.56590759]\n",
      " [-8.06834315]]\n",
      "--------------------------------------------------------------------------------\n",
      " Hessian\n",
      "[[-1.15108197 -0.17071919 -1.5586239   2.52772356  3.56683048]\n",
      " [-0.17071919 -0.34286909  2.46246702  1.24581071  0.32367437]\n",
      " [-1.5586239   2.46246702 -0.06583781 -3.07614388  1.86531067]\n",
      " [ 2.52772356  1.24581071 -3.07614388 -0.60374146 -0.70390081]\n",
      " [ 3.56683048  0.32367437  1.86531067 -0.70390081 -0.70211613]]\n",
      "Verification:\n",
      " [[-1.15108197 -0.17071919 -1.5586239   2.52772356  3.56683048]\n",
      " [-0.17071919 -0.34286909  2.46246702  1.24581071  0.32367437]\n",
      " [-1.5586239   2.46246702 -0.06583781 -3.07614388  1.86531067]\n",
      " [ 2.52772356  1.24581071 -3.07614388 -0.60374146 -0.70390081]\n",
      " [ 3.56683048  0.32367437  1.86531067 -0.70390081 -0.70211613]]\n"
     ]
    }
   ],
   "source": [
    "# Check gradient and hessian of - xT P x + xT Q x\n",
    "x = Tensor(np.random.randn(5, 1), label='x')\n",
    "Q = Tensor(np.random.randn(5, 5), label='Q')\n",
    "P = Tensor(np.random.randn(5, 5), label='P')\n",
    "y = x.T().dot(Q).dot(x)\n",
    "z = x.T().dot(P).dot(x)\n",
    "\n",
    "y = y - z\n",
    "\n",
    "y.backward(secondOrder=True)\n",
    "\n",
    "print(\"Gradient\")\n",
    "\n",
    "print(x.grad)\n",
    "print(Q.data @ x.data + Q.data.T @ x.data - (P.data @ x.data + P.data.T @ x.data))\n",
    "\n",
    "print(f'{'-'*80}\\n Hessian')\n",
    "\n",
    "print(x.get_hessian())\n",
    "print(\"Verification:\\n\", Q.data + Q.data.T - P.data - P.data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 float64\n",
      "(3, 4, 5)\n",
      "(3, 5, 4)\n",
      "False False\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "# Checking shapes of dot products and gradients\n",
    "a = Tensor(np.random.randn(3, 4, 5) * 1.0, label='a')\n",
    "b = Tensor(np.random.randn(3, 5, 4) * 1.0, label='b')\n",
    "\n",
    "c = a.dot(b)\n",
    "\n",
    "# print(c.shape())\n",
    "c.backward(secondOrder=True)\n",
    "\n",
    "print(a.grad.data.dtype, b.grad.data.dtype)\n",
    "print(a.grad.data.shape)\n",
    "print(b.grad.shape())\n",
    "print(isinstance(a.grad, np.ndarray), isinstance(b.grad, np.ndarray))\n",
    "print(isinstance(a.grad, Tensor), isinstance(b.grad, Tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    def __init__(self, layer_sizes):\n",
    "        # layer_sizes is a list of integers where each integer represents the number of neurons in that layer\n",
    "        self.params = []\n",
    "        self.layers = len(layer_sizes) - 1\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            self.params.append(Tensor(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01))\n",
    "            self.params.append(Tensor(np.zeros((1, layer_sizes[i+1]))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        for i in range(self.layers):\n",
    "            x = x.dot(self.params[2*i]) + self.params[2*i+1]\n",
    "            if i < self.layers - 1:\n",
    "                x = x.tanh()\n",
    "        self.z = x\n",
    "        return x\n",
    "\n",
    "    def backward(self, y):\n",
    "        # Backward pass with MSE loss\n",
    "        loss = ((self.z - y) ** 2).sum() / y.data.shape[0]*1.0\n",
    "        loss.backward()  # Use autograd to calculate gradients\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    '''Adam optimizer'''\n",
    "    def __init__(self, model, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = [np.zeros_like(param.data) for param in model.params]\n",
    "        self.v = [np.zeros_like(param.data) for param in model.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, param in enumerate(self.model.params):\n",
    "            if param.grad.shape != param.data.shape:\n",
    "                param.grad = param.grad.sum(axis=0, keepdims=True)\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * param.grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * param.grad**2\n",
    "            m_hat = self.m[i] / (1 - self.beta1**self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2**self.t)\n",
    "            param.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.model.params:\n",
    "            param.grad *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shampoo:\n",
    "    def __init__(self, model, lr=1e-1, momentum=0, weight_decay=0, epsilon=1e-4, diag_cutoff=1e3, update_freq_sched=None, svd_rank=None, newton_num_iter=100, newton_num_iter_max_sv=50):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epsilon = epsilon\n",
    "        self.diag_cutoff = diag_cutoff\n",
    "        self.update_freq_sched = update_freq_sched if update_freq_sched is not None else lambda x: 1\n",
    "        self.svd_rank = svd_rank\n",
    "        self.newton_num_iter = newton_num_iter\n",
    "        self.newton_num_iter_max_sv = newton_num_iter_max_sv\n",
    "        self.state = {}\n",
    "\n",
    "        # Initialize preconditioners\n",
    "        for p in model.params:\n",
    "            self.state[p] = {}\n",
    "            self.state[p]['step'] = 0\n",
    "            if momentum > 0:\n",
    "                self.state[p]['momentum_buffer'] = np.zeros_like(p.data)\n",
    "            \n",
    "            grad_size = p.data.shape\n",
    "            for i, dim in enumerate(grad_size):\n",
    "                if dim > diag_cutoff:\n",
    "                    self.state[p][f'precond_{i}'] = epsilon * np.ones(dim)\n",
    "                    self.state[p][f'inv_precond_{i}'] = np.zeros(dim)\n",
    "                else:\n",
    "                    self.state[p][f'precond_{i}'] = epsilon * np.eye(dim)\n",
    "                    self.state[p][f'inv_precond_{i}'] = np.zeros((dim, dim))\n",
    "\n",
    "    def matrix_pow_svd(self, matrix, power):\n",
    "        \"\"\" Compute approximate power of a matrix using svd \"\"\"\n",
    "        u, s, v = np.linalg.svd(matrix)\n",
    "        rank = self.svd_rank\n",
    "        if rank is not None:\n",
    "            rank = min(rank, min(matrix.shape))\n",
    "            # Truncated SVD\n",
    "            s = s[:rank]\n",
    "            u = u[:, :rank]\n",
    "            v = v[:rank, :]\n",
    "        result = u @ np.diag(s**power) @ v\n",
    "        return result\n",
    "\n",
    "    def max_sv(self, G, error_tolerance=1e-6):\n",
    "        \"\"\"Computes the largest singular value of G using power iteration.\"\"\"\n",
    "        num_iters = self.newton_num_iter_max_sv\n",
    "        n = G.shape[0]\n",
    "        v = np.random.randn(n)\n",
    "        sv = 0\n",
    "\n",
    "        for _ in range(num_iters):\n",
    "            v_hat = v / np.linalg.norm(v)\n",
    "            v = G @ v_hat\n",
    "            new_sv = v_hat @ v\n",
    "            if abs(new_sv - sv) < error_tolerance:\n",
    "                break\n",
    "            sv = new_sv\n",
    "\n",
    "        return sv\n",
    "\n",
    "    def matrix_pow_newton(self, G, power, error_tolerance=1e-5, ridge_epsilon=1e-6):\n",
    "        \"\"\"Computes G^(-1/power) using Coupled Newton Iteration.\"\"\"\n",
    "        num_iters = self.newton_num_iter\n",
    "        mat_size = G.shape[0]\n",
    "        identity = np.eye(mat_size)\n",
    "        max_eigenvalue = self.max_sv(G)\n",
    "        ridge_epsilon = ridge_epsilon * max(max_eigenvalue, 1e-16)\n",
    "        damped_mat_g = G + ridge_epsilon * identity\n",
    "\n",
    "        alpha = -1.0 / power\n",
    "        z = (1 + power) / (2 * np.linalg.norm(damped_mat_g))\n",
    "        mat_m = damped_mat_g * z\n",
    "        mat_h = identity * z**(1 / power)\n",
    "        error = np.max(np.abs(mat_m - identity))\n",
    "\n",
    "        for _ in range(num_iters):\n",
    "            if error < error_tolerance:\n",
    "                break\n",
    "\n",
    "            mat_m_i = (1 - alpha) * identity + alpha * mat_m\n",
    "            mat_h = mat_h @ mat_m_i\n",
    "            mat_m = np.linalg.matrix_power(mat_m_i, power) @ mat_m\n",
    "            error = np.max(np.abs(mat_m - identity))\n",
    "\n",
    "        return mat_h\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.model.params:\n",
    "            # Skip parameters without gradients\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            if p.grad.shape != p.data.shape:\n",
    "                p.grad = p.grad.sum(axis=0, keepdims=True)\n",
    "\n",
    "            # Get the gradient\n",
    "            grad = p.grad if isinstance(p.grad, np.ndarray) else p.grad.data\n",
    "            n_dim = len(grad.shape)\n",
    "            grad_size = grad.shape\n",
    "            momentum = self.momentum\n",
    "            weight_decay = self.weight_decay\n",
    "            state = self.state[p]\n",
    "            \n",
    "            # Update frequency of preconditioner\n",
    "            update_freq = self.update_freq_sched(state['step'])\n",
    "\n",
    "            # Apply momentum and weight decay\n",
    "            if momentum > 0:\n",
    "                grad = grad * (1 - momentum) + state['momentum_buffer'] * momentum\n",
    "            if weight_decay > 0:\n",
    "                grad = grad + p.data * weight_decay\n",
    "\n",
    "            for i, dim in enumerate(grad_size):\n",
    "                # Get the preconditioner\n",
    "                precond = state[f'precond_{i}']\n",
    "                inv_precond_pow = state[f'inv_precond_{i}']\n",
    "\n",
    "                grad = np.transpose(grad, (i, *range(i), *range(i + 1, n_dim)))\n",
    "                grad_size_t = grad.shape\n",
    "                grad = grad.reshape(dim, -1)\n",
    "\n",
    "                # Update the preconditioner\n",
    "                if dim > self.diag_cutoff:\n",
    "                    precond += np.diagonal(grad @ grad.T)\n",
    "                    if state['step'] % update_freq == 0:\n",
    "                        inv_precond_pow = np.power(precond, -1 / (2*n_dim))\n",
    "                else:\n",
    "                    precond += grad @ grad.T\n",
    "                    if state['step'] % update_freq == 0:\n",
    "                        # inv_precond_pow = self.matrix_pow_svd(precond, -1 / (2*n_dim))\n",
    "                        inv_precond_pow = self.matrix_pow_newton(precond, 2*n_dim)\n",
    "\n",
    "                # Apply preconditioning\n",
    "                if i == n_dim - 1:\n",
    "                    if dim > self.diag_cutoff:\n",
    "                        grad = grad.T @ np.diag(inv_precond_pow)\n",
    "                    else:\n",
    "                        grad = grad.T @ inv_precond_pow\n",
    "                    grad = grad.reshape(grad_size)\n",
    "\n",
    "                else:\n",
    "                    if dim > self.diag_cutoff:\n",
    "                        grad = np.diag(inv_precond_pow) @ grad\n",
    "                    else:\n",
    "                        grad = inv_precond_pow @ grad\n",
    "                    grad = grad.reshape(grad_size_t)\n",
    "                \n",
    "            # Update the parameters and state\n",
    "            state['step'] += 1\n",
    "            state['momentum_buffer'] = grad\n",
    "            p.data = p.data - grad * self.lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        ''' Reset gradients after step '''\n",
    "        for param in self.model.params:\n",
    "            param.grad *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaturalGradientDescent:\n",
    "    def __init__(self, model, lr=1e-1, tol=1e-5, max_iter=1000):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def compute_fisher_vector_product(self, grad):\n",
    "        \"\"\"Compute Fisher as a vector product\"\"\"\n",
    "        original_shape = grad.shape\n",
    "        # Flatten to 2D: (batch_size, -1)\n",
    "        grad_flat = grad.reshape(original_shape[0], -1)\n",
    "        # Compute Fisher matrix\n",
    "        fisher_matrix = grad_flat @ grad_flat.T + 1e-5 * np.eye(grad_flat.shape[0])\n",
    "        return fisher_matrix, original_shape\n",
    "    \n",
    "    def conjugate_gradient(self, fisher_matrix, grad, tol, max_iter):\n",
    "        \"\"\"Conjugate gradient method to solve for F^-1 @ grad\"\"\"\n",
    "        original_shape = grad.shape\n",
    "        grad_flat = grad.reshape(original_shape[0], -1)\n",
    "        \n",
    "        x = np.zeros_like(grad_flat)\n",
    "        r = grad_flat.copy()\n",
    "        p = r.copy()\n",
    "        rs_old = np.sum(r * r)\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            Ap = fisher_matrix @ p\n",
    "            alpha = rs_old / (np.sum(p * Ap) + 1e-8)\n",
    "            x += alpha * p\n",
    "            r -= alpha * Ap\n",
    "            rs_new = np.sum(r * r)\n",
    "\n",
    "            if np.sqrt(rs_new) < tol:\n",
    "                break\n",
    "\n",
    "            p = r + (rs_new / rs_old) * p\n",
    "            rs_old = rs_new\n",
    "\n",
    "        # Reshape solution back to original dimensions\n",
    "        return x.reshape(original_shape)\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.model.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            if p.grad.shape != p.data.shape:\n",
    "                p.grad = p.grad.sum(axis=0, keepdims=True)\n",
    "            grad = p.grad if isinstance(p.grad, np.ndarray) else p.grad.data\n",
    "\n",
    "            fisher_matrix, original_shape = self.compute_fisher_vector_product(grad)\n",
    "            natural_grad = self.conjugate_gradient(fisher_matrix, grad, self.tol, self.max_iter)\n",
    "            p.data = p.data - natural_grad * self.lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.model.params:\n",
    "            param.grad *= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MLP on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network for mnist\n",
    "np.random.seed(0)\n",
    "mlp = SimpleMLP([784, 128, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_freq_sched(step):\n",
    "    return 1\n",
    "optimizer = Shampoo(mlp,\n",
    "    lr=1e-1,\n",
    "    epsilon=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    diag_cutoff=1e1,\n",
    "    update_freq_sched=update_freq_sched,\n",
    "    svd_rank=None,\n",
    "    newton_num_iter=10,\n",
    "    newton_num_iter_max_sv=10\n",
    ")\n",
    "# optimizer = Adam(mlp, lr=1e-2)\n",
    "optimizer = NaturalGradientDescent(mlp, lr=1.5e-2, tol=1e-5, max_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/60000: Loss: 1.0017088234225717, Accuracy: 5.6640625%\n",
      "Batch 12800/60000: Loss: 0.0013755052859631182, Accuracy: 80.078125%\n",
      "Batch 25600/60000: Loss: 0.0012188575437103299, Accuracy: 81.4453125%\n",
      "Batch 38400/60000: Loss: 0.0011387113280659088, Accuracy: 79.6875%\n",
      "Batch 51200/60000: Loss: 0.0010720790613857618, Accuracy: 82.421875%\n",
      "Epoch 1/1: Loss: 0.0010393954815031475\n"
     ]
    }
   ],
   "source": [
    "# Batch gradient descent with train_data, train_labels\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    batch_size = 512\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        bt_shape = train_data[i:i+batch_size].shape\n",
    "        x = Tensor(train_data[i:i+batch_size].reshape(bt_shape[0], -1))\n",
    "        \n",
    "        # Convert labels to one-hot encoding\n",
    "        y_onehot = np.zeros((bt_shape[0], 10))\n",
    "        y_onehot[np.arange(bt_shape[0]), train_labels[i:i+bt_shape[0]]] = 1\n",
    "        y = Tensor(y_onehot)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = mlp.forward(x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = mlp.backward(y)\n",
    "        total_loss += loss.data\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i % 100 == 0):\n",
    "            # Calculate accuracy\n",
    "            y_pred_labels = np.argmax(y_pred.data, axis=1)\n",
    "            accuracy = np.mean(y_pred_labels == train_labels[i:i+bt_shape[0]])\n",
    "            print(f'Batch {i}/{len(train_data)}: Loss: {total_loss / (i+1)}, Accuracy: {accuracy*100}%')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}: Loss: {total_loss / len(train_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.37\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on test set\n",
    "x = Tensor(test_data.reshape(len(test_data), -1))\n",
    "y = Tensor(test_labels.reshape(len(test_labels), -1))\n",
    "y_pred = mlp.forward(x)\n",
    "y_pred = np.argmax(y_pred.data, axis=1)\n",
    "accuracy = np.mean(y_pred == test_labels)\n",
    "print(f'Accuracy: {accuracy*100}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
